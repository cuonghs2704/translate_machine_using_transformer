{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSk1awEJmAsQ"
      },
      "outputs": [],
      "source": [
        "!pip install pyvi\n",
        "\n",
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ja_core_news_sm\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare DATA "
      ],
      "metadata": {
        "id": "J47JiR3Rm8Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read data\n",
        "import string\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "vi_input = []\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/NLP/Data/VN.txt\") as f:\n",
        "  for line in f:\n",
        "    line = line.replace('  ', ' ').lower()\n",
        "    vi_input.append(line.strip())\n",
        "\n",
        "ja_input = []\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/NLP/Data/JA.txt\") as f:\n",
        "  for line in f:\n",
        "    ja_input.append(line.strip())\n",
        "\n",
        "# Thêm token đánh dấu điểm bắt đầu và kết thúc của câu vào mỗi câu trong ngôn ngữ đích\n",
        "eos = '<eos>'\n",
        "bos = '<bos>'\n",
        "\n",
        "from pyvi import ViTokenizer\n",
        "vi_input_tokenize = [ViTokenizer.tokenize(i).split() for i in vi_input]\n",
        "for i in range(len(vi_input_tokenize)):\n",
        "  vi_input_tokenize[i].insert(0, bos)\n",
        "  vi_input_tokenize[i].insert(len(vi_input_tokenize[i]), eos)\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"ja_core_news_sm\")\n",
        "ja_input_tokenize = [[] for i in range(len(ja_input))]\n",
        "for i in range(len(ja_input)):\n",
        "  doc = nlp(ja_input[i])\n",
        "  for token in doc:\n",
        "    ja_input_tokenize[i].append(str(token))\n",
        "\n",
        "#build vocab\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "ja_tokenizer = Tokenizer(oov_token = '<oov>')\n",
        "ja_tokenizer.fit_on_texts(ja_input_tokenize)\n",
        "ja_vocabulary = ja_tokenizer.word_index\n",
        "\n",
        "\n",
        "vi_tokenizer = Tokenizer()\n",
        "vi_tokenizer.fit_on_texts(vi_input_tokenize)\n",
        "vi_vocabulary = vi_tokenizer.word_index\n",
        "\n",
        "ja_vocabulary_reverse = {}\n",
        "for key, value in ja_tokenizer.word_index.items():\n",
        "  ja_vocabulary_reverse[value] = key\n",
        "ja_vocabulary_reverse[0] = ''\n",
        "\n",
        "vi_vocabulary_reverse = {}\n",
        "for key, value in vi_tokenizer.word_index.items():\n",
        "  vi_vocabulary_reverse[value] = key\n",
        "vi_vocabulary_reverse[0] = ''\n",
        "vi_vocab_size = len(vi_vocabulary)\n",
        "ja_vocab_size = len(ja_vocabulary)\n",
        "\n",
        "\n",
        "#padding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "ja_sequence = ja_tokenizer.texts_to_sequences(ja_input_tokenize)\n",
        "jamaxlen = max([len(i) for i in ja_sequence]) \n",
        "ja_sequence = pad_sequences(ja_sequence, maxlen = jamaxlen, padding = 'post')\n",
        "vi_sequence = vi_tokenizer.texts_to_sequences(vi_input_tokenize)\n",
        "vimaxlen = max([len(i) for i in vi_sequence])\n",
        "vi_sequence = pad_sequences(vi_sequence, maxlen = vimaxlen, padding = 'post')\n",
        "\n",
        "PAD_IDX = 0\n",
        "BOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "#torch.utils.data.DataLoader\n",
        "#jamaxlen, vimaxlen, ja_vocab_size, vi_vocab_size, \n",
        "#onehot = nn.functional.one_hot(input_vi, num_classes)"
      ],
      "metadata": {
        "id": "wzNjoLvqm_Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VFP78kS3jQ0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "syo56-_-rBsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model, dropout_p, max_len):\n",
        "        super().__init__()\n",
        "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "        # max_len determines how far the position can have an effect on a token (window)\n",
        "        \n",
        "        # Info\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        \n",
        "        # Encoding - From formula\n",
        "        pos_encoding = torch.zeros(max_len, dim_model)\n",
        "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
        "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
        "        \n",
        "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
        "        \n",
        "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
        "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
        "        \n",
        "        # Saving buffer (same as parameter without gradients needed)\n",
        "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
        "        \n",
        "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
        "        # Residual connection + pos encoding\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
      ],
      "metadata": {
        "id": "5iEDnLxArDvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vi_vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dml6E55kwALt",
        "outputId": "0387945d-fdb4-4a2a-9a81-e77e84d35d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1634"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "from torch import Tensor\n",
        "class Transformer(nn.Module): \n",
        "    # Constructor\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_tokens_vi,\n",
        "        num_tokens_ja,\n",
        "        dim_model,\n",
        "        num_heads,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        dropout_p,\n",
        "    ):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        # INFO\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.dim_model = dim_model\n",
        "\n",
        "        # LAYERS\n",
        "        self.positional_encoder_vi = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=num_tokens_vi\n",
        "        )\n",
        "        self.positional_encoder_ja = PositionalEncoding(\n",
        "            dim_model=dim_model, dropout_p=dropout_p, max_len=num_tokens_ja\n",
        "        )\n",
        "        self.embedding_vi = nn.Embedding(num_tokens_vi, dim_model)\n",
        "        self.embedding_ja = nn.Embedding(num_tokens_ja, dim_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=dim_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout_p,\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(dim_model, num_tokens_vi),\n",
        "            nn.Softmax(dim=2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask: Tensor,\n",
        "                tgt_mask: Tensor, src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        # Src size must be (batch_size, src sequence length)\n",
        "        # Tgt size must be (batch_size, tgt sequence length)\n",
        "        \n",
        "\n",
        "        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)\n",
        "        src = self.embedding_ja(src) * math.sqrt(self.dim_model)\n",
        "        tgt = self.embedding_vi(tgt) * math.sqrt(self.dim_model)\n",
        "        src = self.positional_encoder_ja(src)\n",
        "        tgt = self.positional_encoder_vi(tgt)\n",
        "        \n",
        "       \n",
        "        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute\n",
        "        # to obtain size (sequence length, batch_size, dim_model),\n",
        "        src = src.permute(1,0,2)\n",
        "        tgt = tgt.permute(1,0,2)\n",
        "      \n",
        "        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)\n",
        "        transformer_out = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask, \n",
        "                                           src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
        "        out = self.out(transformer_out)\n",
        "        out = out.permute(1,0,2)\n",
        "        return out\n",
        "      \n",
        "    def get_tgt_mask(self, size) -> torch.tensor:\n",
        "        # Generates a squeare matrix where the each row allows one word more to be seen\n",
        "        mask = torch.tril(torch.ones(size, size) == 1) # Lower triangular matrix\n",
        "        mask = mask.float()\n",
        "        mask = mask.masked_fill(mask == 0, float('-inf')) # Convert zeros to -inf\n",
        "        mask = mask.masked_fill(mask == 1, float(0.0)) # Convert ones to 0\n",
        "        \n",
        "        # EX for size=5:\n",
        "        # [[0., -inf, -inf, -inf, -inf],\n",
        "        #  [0.,   0., -inf, -inf, -inf],\n",
        "        #  [0.,   0.,   0., -inf, -inf],\n",
        "        #  [0.,   0.,   0.,   0., -inf],\n",
        "        #  [0.,   0.,   0.,   0.,   0.]]\n",
        "        \n",
        "        return mask\n",
        "    \n",
        "    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -> torch.tensor:\n",
        "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
        "        # [False, False, False, True, True, True]\n",
        "        return (matrix == pad_token)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = Transformer(num_tokens_vi=vi_vocab_size, num_tokens_ja=ja_vocab_size, dim_model=256, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, dropout_p=0.1).to(device)\n",
        "opt = torch.optim.Adam(\n",
        "    model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9\n",
        ")\n",
        "loss_fn = nn.BCELoss()\n"
      ],
      "metadata": {
        "id": "Z-kW3oUmt21T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "  def __init__(self, ja, vi, num_vocab):\n",
        "    self.ja = torch.Tensor(ja).to(torch.int64)\n",
        "    self.vi = torch.Tensor(vi).to(torch.int64)\n",
        "    self.num_vocab = num_vocab\n",
        "  def __getitem__(self, i):\n",
        "    vi_onehot = self.vi[i]\n",
        "    vi_onehot = nn.functional.one_hot(vi_onehot, self.num_vocab)\n",
        "    \n",
        "    return [self.ja[i], self.vi[i][:-1], vi_onehot[1:]]\n",
        "dataset = Dataset(ja_sequence[:1700], vi_sequence[:1700], vi_vocab_size)\n",
        "val_dataset = Dataset(ja_sequence[1700:], vi_sequence[1700:], vi_vocab_size)"
      ],
      "metadata": {
        "id": "5CiWq4fAosOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class Dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset, batch_size, size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.size = size\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # collect batch data\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
        "        return tuple(batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size // self.batch_size\n",
        "train_dataloader = Dataloader(dataset, 128, len(ja_sequence[:1700]))\n",
        "val_dataloader = Dataloader(val_dataset, 128, len(vi_sequence[1700:]))"
      ],
      "metadata": {
        "id": "bCDqj8uz_rAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBgeZ4_SVYDa",
        "outputId": "bfe99203-cbba-49f2-b2e7-7f01e7667a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 63)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 128\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "  src_seq_len = src.shape[1]\n",
        "  tgt_seq_len = tgt.shape[1]\n",
        "\n",
        "  tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "  src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
        "\n",
        "  src_padding_mask = (src == PAD_IDX)#.transpose(0, 1)\n",
        "  tgt_padding_mask = (tgt == PAD_IDX)#.transpose(0, 1)\n",
        "  return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
        "def train_loop(model, opt, loss_fn, dataloader):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i in range(len(dataloader)):\n",
        "    src = torch.Tensor(dataloader[i][0]).to(torch.int64).to(device)\n",
        "    tgt_i = torch.Tensor(dataloader[i][1]).to(torch.int64).to(device)\n",
        "    tgt_o =torch.Tensor(dataloader[i][2]).to(torch.float).to(device) \n",
        "\n",
        "    #mask\n",
        "    src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_i)\n",
        "    \n",
        "    #print(src_mask.shape, src_padding_mask.shape, tgt_padding_mask.shape, tgt_mask.shape)\n",
        "    pre = model(src, tgt_i, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "    loss = loss_fn(pre, tgt_o) \n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    total_loss += loss.detach().item()\n",
        "  \n",
        "  return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "gSDrucw71TQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.classification import BinaryAccuracy\n",
        "def validation_loop(model, loss_fn, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for i in range(len(dataloader)):\n",
        "        for batch in dataloader:\n",
        "            src = torch.Tensor(dataloader[i][0]).to(torch.int64).to(device)\n",
        "            tgt_i = torch.Tensor(dataloader[i][1]).to(torch.int64).to(device)\n",
        "            tgt_o =torch.Tensor(dataloader[i][2]).to(torch.float).to(device) \n",
        "            \n",
        "            # Get mask to mask out the next words\n",
        "          \n",
        "            \n",
        "\n",
        "            # Standard training except we pass in y_input and src_mask\n",
        "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_i)\n",
        "    \n",
        "    #print(src_mask.shape, src_padding_mask.shape, tgt_padding_mask.shape, tgt_mask.shape)\n",
        "            pred = model(src, tgt_i, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "            #print(pred.shape, tgt_o.shape)\n",
        "            # Permute pred to have batch size first again\n",
        "               \n",
        "            loss = loss_fn(pred, tgt_o)\n",
        "            total_loss += loss.detach().item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "r50TRRGw5daq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, validation_loss_list = [], []\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "      print(\"-\"*25, f\"Epoch {epoch + 1}\",\"-\"*25)\n",
        "      tgt_mask = torch.nn.Transformer().generate_square_subsequent_mask(44).to(device).transpose(0,1)  \n",
        "      train_loss = train_loop(model, opt, loss_fn, train_dataloader)\n",
        "      train_loss_list += [train_loss]\n",
        "        \n",
        "      validation_loss = validation_loop(model, loss_fn, val_dataloader)\n",
        "      validation_loss_list += [validation_loss]\n",
        "        \n",
        "      print(f\"Training loss: {10000*train_loss:.4f}\")\n",
        "      print(f\"Validation loss: {10000*validation_loss:.4f}\")\n",
        "      print()"
      ],
      "metadata": {
        "id": "SG8UpBmCAw7-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_input = torch.Tensor(val_dataloader[0][0]).to(torch.int64).to(device)\n",
        "tgt_input = torch.Tensor(val_dataloader[0][1]).to(torch.int64).to(device)\n",
        "tgt_output = torch.Tensor(val_dataloader[0][2]).to(torch.int64).to(device)\n",
        "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_input, tgt_input)\n",
        "print(src_input.shape, tgt_input.shape, tgt_mask.shape, tgt_output.shape)\n",
        "output = model(src_input, tgt_input, src_mask, tgt_mask,\n",
        "                                src_padding_mask, tgt_padding_mask, src_padding_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAe8gJSd7tWW",
        "outputId": "b01ccce2-81b3-4f9f-d9e3-6bdb9f1d47f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 63]) torch.Size([128, 44]) torch.Size([44, 44]) torch.Size([128, 44, 1634])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(output[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl3F91H5-5_V",
        "outputId": "bdc9ca5a-7185-44ab-9a7c-c397b2b2f3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0000, device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre = [np.argmax(i.cpu().detach().numpy()) for i in output[0]]\n",
        "tar = [np.argmax(t.cpu().detach().numpy()) for t in tgt_output[0]]"
      ],
      "metadata": {
        "id": "Ai2gAocQ-G4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pre[0:10], tar[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c01sMyeRI5yN",
        "outputId": "7b4b34ba-364c-4517-b4cd-eafd4c18e553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[72, 241, 64, 4, 1299, 1300, 25, 40, 466, 3] [72, 241, 64, 4, 1299, 1300, 25, 40, 466, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vi_sent_pred = [vi_vocabulary_reverse[i] for i in pre]\n",
        "vi_sent = [vi_vocabulary_reverse[i] for i in tar]"
      ],
      "metadata": {
        "id": "UASnOyPQQ_GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vi_sent_pred, vi_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wU033HJLlaZ",
        "outputId": "640c07ed-73c3-4537-a23e-8c9fa320b0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['con', 'chó', 'nhà', 'tôi', 'chôn', 'xương', 'ở', 'trong', 'vườn', '.', '<eos>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''] ['con', 'chó', 'nhà', 'tôi', 'chôn', 'xương', 'ở', 'trong', 'vườn', '.', '<eos>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    }
  ]
}